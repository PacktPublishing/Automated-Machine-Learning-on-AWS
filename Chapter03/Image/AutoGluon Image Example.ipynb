{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon Image Example\n",
    ">__NOTE:__ Make sure to use the Pyton 3 (Data Science) Jupyter Kernel.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Intalling the Image Build CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!{sys.executable} -m pip install -U pip sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the AutoGluon Training/Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.vision import ImagePredictor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "prefix = \"/opt/ml\"\n",
    "input_path = os.path.join(prefix, \"input/data\")\n",
    "output_path = os.path.join(prefix, \"output\")\n",
    "model_path = os.path.join(prefix, \"model\")\n",
    "param_path = os.path.join(prefix, \"input/config/hyperparameters.json\")\n",
    "\n",
    "\n",
    "def train(params):\n",
    "    time_limit = int(params[\"time_limit\"])\n",
    "    presets = \"\".join([str(i) for i in list(params[\"presets\"])])\n",
    "    channel_name = \"training\"\n",
    "    training_path = os.path.join(input_path, channel_name)\n",
    "    training_dataset = ImagePredictor.Dataset.from_folder(training_path)\n",
    "    predictor = ImagePredictor().fit(training_dataset, time_limit=time_limit, presets=presets)\n",
    "    table = pd.DataFrame.from_dict(predictor.fit_summary())\n",
    "    table.to_csv(os.path.join(model_path, \"FitSummary.csv\"), index=False)\n",
    "    predictor.save(os.path.join(model_path, \"ImagePredictor.Autogluon\"))\n",
    "    return \"AutoGluon Job Complete\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading Parameters\\n\")\n",
    "    with open(param_path) as f:\n",
    "        params = json.load(f)\n",
    "    print(\"Training Models\\n\")\n",
    "    result = train(params)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Image Build Instructions (Dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/mxnet-training:1.8.0-gpu-py37-cu110-ubuntu16.04\n",
    "RUN pip install -U pip wheel setuptools\n",
    "RUN pip install autogluon\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "COPY train.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Build Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "aws_region = sagemaker.Session().boto_session.region_name\n",
    "!sm-docker build --build-arg REGION={aws_region} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AutoGluon Experiment\n",
    "\n",
    "### Download the Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip\"\n",
    "with urllib.request.urlopen(dataset_url) as rps_zipfile:\n",
    "    with zipfile.ZipFile(io.BytesIO(rps_zipfile.read())) as z:\n",
    "        z.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters\n",
    "\n",
    ">__NOTE:__ Update the `image_uri` parameter with the _Image URI_ output the __Container Build Process__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import datetime\n",
    "\n",
    "image_uri = \"<Enter the Image URI from the sm-docker output>\"\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "job_version = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]\n",
    "job_name = f\"autogluon-image-{job_version}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AutoGluon Estimator\n",
    "\n",
    ">__NOTE:__ To leverage [Managed Spot Training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) to further resuce training costs, uncomment the lines in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "autogluon = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    output_path=f\"s3://{bucket}/{job_name}\",\n",
    "    base_job_name=job_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    hyperparameters={\n",
    "        \"presets\": \"medium_quality_faster_train\",\n",
    "        \"time_limit\": \"600\",\n",
    "        \"bucket\": bucket,\n",
    "        \"training_job\": job_name\n",
    "    },\n",
    "    volume_size=50,\n",
    "#     use_spot_instances=True,\n",
    "#     max_wait=3600,\n",
    "#     max_run=8*3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogluon.fit(\n",
    "    inputs={\n",
    "        \"training\": session.upload_data(\n",
    "            \"data/rps\",\n",
    "            bucket=bucket,\n",
    "            key_prefix=f\"{job_name}/input\"\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Results\n",
    "\n",
    "#### Download Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader.download(autogluon.model_data, \"./\")\n",
    "!tar xfz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "df = pd.read_csv(\"FitSummary.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
