{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon Tabular Example\n",
    ">__NOTE:__ Make sure to use the Pyton 3 (Data Science) Jupyter Kernel.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Intalling the Image Build CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!{sys.executable} -m pip install -U pip sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the AutoGluon Training/Testing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "prefix = \"/opt/ml\"\n",
    "input_path = os.path.join(prefix, \"input/data\")\n",
    "output_path = os.path.join(prefix, \"output\")\n",
    "model_path = os.path.join(prefix, \"model\")\n",
    "param_path = os.path.join(prefix, 'input/config/hyperparameters.json')\n",
    "\n",
    "\n",
    "def train(params):\n",
    "    label = params[\"label\"]\n",
    "    channel_name = \"training\"\n",
    "    training_path = os.path.join(input_path, channel_name)\n",
    "    training_dataset = TabularDataset(os.path.join(training_path, \"training.csv\"))\n",
    "    predictor = TabularPredictor(label=label, path=model_path).fit(training_dataset)\n",
    "    with open(os.path.join(model_path, \"Fit_Summary.txt\"), \"w\") as f:\n",
    "        print(predictor.fit_summary(), file=f)\n",
    "    return predictor\n",
    "    \n",
    "\n",
    "def test(params, predictor):\n",
    "    label = params[\"label\"]\n",
    "    channel_name = \"testing\"\n",
    "    testing_path = os.path.join(input_path, channel_name)\n",
    "    testing_dataset = TabularDataset(os.path.join(testing_path, \"testing.csv\"))\n",
    "    ground_truth = testing_dataset[label]\n",
    "    testing_data = testing_dataset.drop(columns=label)\n",
    "    predictions = predictor.predict(testing_data)\n",
    "    with open(os.path.join(model_path, \"Model_Evaluation.txt\"), \"w\") as f:\n",
    "        print(\n",
    "            json.dumps(\n",
    "                predictor.evaluate_predictions(\n",
    "                    y_true=ground_truth,\n",
    "                    y_pred=predictions,\n",
    "                    auxiliary_metrics=True\n",
    "                ),\n",
    "                indent=4\n",
    "            ),\n",
    "            file=f\n",
    "        )\n",
    "    leaderboard = predictor.leaderboard(testing_dataset, silent=True)\n",
    "    leaderboard.to_csv(os.path.join(model_path, \"Leaderboard.csv\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading Parameters\\n\")\n",
    "    with open(param_path) as f:\n",
    "        params = json.load(f)\n",
    "    print(\"Training Models\\n\")\n",
    "    predictor = train(params)\n",
    "    print(\"Testig Models\\n\")\n",
    "    test(params, predictor)\n",
    "    print(\"AutoGluon Job Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Image Build Instructions (Dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/autogluon-training:0.3.1-cpu-py37-ubuntu18.04\n",
    "RUN pip install -U pip\n",
    "RUN pip install bokeh==2.0.1\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "COPY train.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Build Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "aws_region = sagemaker.Session().boto_session.region_name\n",
    "!sm-docker build --build-arg REGION={aws_region} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AutoGluon Experiment\n",
    "\n",
    "### Download the Abalone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole_weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"rings\"]\n",
    "abalone_data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\", names=column_names)\n",
    "training_data, testing_data = train_test_split(abalone_data, test_size=0.1)\n",
    "training_data.to_csv(\"training.csv\")\n",
    "testing_data.to_csv(\"testing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters\n",
    "\n",
    ">__NOTE:__ Update the `image_uri` parameter with the _Image URI_ output the __Container Build Process__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import datetime\n",
    "\n",
    "image_uri = \"<Enter the Image URI from the sm-docker output>\"\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "job_version = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')[:-3]\n",
    "job_name = f\"abalone-autogluon-{job_version}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AutoGluon Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "autogluon = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    output_path=f\"s3://{bucket}/{job_name}\",\n",
    "    base_job_name=job_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    hyperparameters={\n",
    "        \"label\": \"rings\",\n",
    "        \"bucket\": bucket,\n",
    "        \"training_job\": job_name\n",
    "    },\n",
    "    volume_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogluon.fit(\n",
    "    inputs={\n",
    "        \"training\": session.upload_data(\n",
    "            \"training.csv\",\n",
    "            bucket=bucket,\n",
    "            key_prefix=f\"{job_name}/input\"\n",
    "        ),\n",
    "        \"testing\": session.upload_data(\n",
    "            \"testing.csv\",\n",
    "            bucket=bucket,\n",
    "            key_prefix=f\"{job_name}/input\"\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Results\n",
    "\n",
    "#### Download Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir extract\n",
    "sagemaker.s3.S3Downloader.download(autogluon.model_data, \"./\")\n",
    "!tar xfz ./model.tar.gz -C extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Model Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./extract/Leaderboard.csv\")\n",
    "df = df.filter([\"model\",\"score_test\", \"score_val\"]).sort_values(by=\"score_val\", ascending=False).reset_index().drop(columns=\"index\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.HTML(filename=\"./extract/SummaryOfModels.html\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
