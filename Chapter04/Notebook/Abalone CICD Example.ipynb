{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone CI/CD Model Example\n",
    "\n",
    "## Configuring the Model Preproceesing, Training and Evaluation Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "import pathlib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "prefix = \"/opt/ml\"\n",
    "processing_path = os.path.join(prefix, \"processing\")\n",
    "preprocessing_input_path = os.path.join(processing_path, \"input/data\")\n",
    "preprocessing_output_path = os.path.join(processing_path, \"output\")\n",
    "training_input_path = os.path.join(prefix, \"input/data\")\n",
    "evaluation_input_path = os.path.join(processing_path, \"input\")\n",
    "evaluation_output_path = os.path.join(processing_path, \"output/evaluation\")\n",
    "output_path = os.path.join(prefix, \"output\")\n",
    "model_path = os.path.join(prefix, \"model\")\n",
    "param_path = os.path.join(prefix, \"input/config/hyperparameters.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a model.py\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    print(\"Preprocessing mode\")\n",
    "    column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole_weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"rings\"]\n",
    "    try:\n",
    "        print(\"Loading 'raw' data\")\n",
    "        abalone_data = pd.read_csv(os.path.join(preprocessing_input_path, \"abalone.data\"), names=column_names)\n",
    "        data = abalone_data[[\"rings\", \"sex\", \"length\", \"diameter\", \"height\", \"whole_weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\"]]\n",
    "        y = data.rings.values.reshape(len(data), 1)\n",
    "        del data[\"rings\"]\n",
    "        data = pd.get_dummies(data).to_numpy()\n",
    "        X = np.concatenate((y, data), axis=1)\n",
    "        training, validation, testing = np.split(X, [int(.8*len(X)), int(.95*len(X))])\n",
    "        pd.DataFrame(training).to_csv(os.path.join(preprocessing_output_path, \"training/training.csv\"), header=False, index=False)\n",
    "        pd.DataFrame(validation).to_csv(os.path.join(preprocessing_output_path, \"training/validation.csv\"), header=False, index=False)\n",
    "        pd.DataFrame(testing).to_csv(os.path.join(preprocessing_output_path, \"testing/testing.csv\"), header=False, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, \"failure\"), \"w\") as f:\n",
    "            f.write(\"Exception during preprocessing: {}\".format(str(e)+'\\\\n'+trc))\n",
    "        print(\"Exception during preprocessing: {}\".format(str(e)+'\\\\n'+trc), file=sys.stderr)\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a model.py\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Training mode\")\n",
    "    try:\n",
    "        channel_name = \"training\"\n",
    "        training_path = os.path.join(training_input_path, channel_name)\n",
    "        params = {}\n",
    "        with open(param_path, \"r\") as f:\n",
    "            is_float = re.compile(r'^\\d+(?:\\.\\d+)$')\n",
    "            is_integer = re.compile(r'^\\d+$')\n",
    "            for key,value in json.load(f).items():\n",
    "                if is_float.match(value) is not None:\n",
    "                    value = float(value)\n",
    "                elif is_integer.match(value) is not None:\n",
    "                    value = int(value)\n",
    "                params[key] = value\n",
    "\n",
    "        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "        if len(input_files) == 0:\n",
    "            raise ValueError((f\"There are no files in {training_path}.\\\\n\" +\n",
    "                              f\"This usually indicates that the channel ({channel_name}) was incorrectly specified,\\\\n\" +\n",
    "                              \"the data specification in S3 was incorrectly specified or the role specified\\\\n\" +\n",
    "                              \"does not have permission to access the data.\"))\n",
    "        column_names = [\"rings\", \"length\", \"diameter\", \"height\", \"whole weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"sex_F\", \"sex_I\", \"sex_M\"]\n",
    "        train_data = pd.read_csv(os.path.join(training_path, \"training.csv\"), sep=',', names=column_names)\n",
    "        val_data = pd.read_csv(os.path.join(training_path, \"validation.csv\"), sep=',', names=column_names)\n",
    "        train_y = train_data[\"rings\"].to_numpy()\n",
    "        train_X = train_data.drop([\"rings\"], axis=1).to_numpy()\n",
    "        val_y = val_data[\"rings\"].to_numpy()\n",
    "        val_X = val_data.drop([\"rings\"], axis=1).to_numpy()\n",
    "        train_X = preprocessing.normalize(train_X)\n",
    "        val_X = preprocessing.normalize(val_X)\n",
    "        network_layers = [\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=\"normal\", input_dim=10),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dense(1, activation=\"linear\")\n",
    "        ]\n",
    "        model = Sequential(network_layers)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\", \"accuracy\"])\n",
    "        model.summary()\n",
    "        model.fit(train_X, train_y, validation_data=(val_X, val_y),\n",
    "                  batch_size=params.get(\"batch_size\"), epochs=params.get(\"epochs\"),\n",
    "                  shuffle=True, verbose=1\n",
    "        )\n",
    "        print(\"Saving Model\")\n",
    "        model.save(filepath=os.path.join(model_path, \"model.h5\"), overwrite=True, include_optimizer=False, save_format=\"h5\")\n",
    "\n",
    "    except Exception as e:\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, \"failure\"), \"w\") as f:\n",
    "            f.write(\"Exception during training: {}\".format(str(e) + '\\\\n' + trc))\n",
    "        print(\"Exception during training: {}\".format(str(e) + '\\\\n' + trc), file=sys.stderr)\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a model.py\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    print(\"Load Pre-Trained Model\")\n",
    "    model_path = os.path.join(evaluation_input_path, \"model/model.tar.gz\")\n",
    "    with tarfile.open(model_path) as tar_file:\n",
    "        tar_file.extractall(\".\")\n",
    "    model = tf.keras.models.load_model(\"model.h5\")\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_report(rmse, mse):\n",
    "    print(\"Saving Evaluation Report\")\n",
    "    report = {\n",
    "        'regression_metrics': {\n",
    "            'rmse': {\n",
    "                'value': rmse\n",
    "            },\n",
    "            'mse': {\n",
    "                'value': mse,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    pathlib.Path(evaluation_output_path).mkdir(parents=True, exist_ok=True)\n",
    "    evaluation_path = f\"{evaluation_output_path}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report))\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    try:\n",
    "        print(\"Evaluation mode\")\n",
    "        model = load_model()\n",
    "        truths = []\n",
    "        predictions = []\n",
    "        column_names = [\"rings\", \"length\", \"diameter\", \"height\", \"whole_weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"sex_F\", \"sex_I\", \"sex_M\"]\n",
    "        data_path = os.path.join(evaluation_input_path, \"data/testing.csv\")\n",
    "        data = pd.read_csv(data_path, names=column_names)\n",
    "        y = data[\"rings\"].to_numpy()\n",
    "        X = data.drop([\"rings\"], axis=1).to_numpy()\n",
    "        X = preprocessing.normalize(X)\n",
    "        for row in range(len(X)):\n",
    "            payload = [X[row].tolist()]\n",
    "            result = model.predict(payload)\n",
    "            print(f\"Result: {result[0][0]}\")\n",
    "            predictions.append(float(result[0][0]))\n",
    "            truths.append(float(y[row]))\n",
    "        mse = mean_squared_error(truths, predictions)\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        rmse = mean_squared_error(truths, predictions, squared=False)\n",
    "        print(f\"Root Mean Squared Error: {rmse}\")\n",
    "        save_report(rmse, mse)\n",
    "        \n",
    "    except Exception as e:\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, \"failure\"), \"w\") as f:\n",
    "            f.write(\"Exception during evaluation: {}\".format(str(e) + '\\\\n' + trc))\n",
    "        print(\"Exception during evaluation: {}\".format(str(e) + '\\\\n' + trc), file=sys.stderr)\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the Application\n",
    "\n",
    "### Container entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "import traceback\n",
    "import flask\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import tarfile\n",
    "import model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "prefix = \"/opt/ml\"\n",
    "model_path = os.path.join(prefix, \"model\")\n",
    "sys.path.insert(0,model_path)\n",
    "model_cache = {}\n",
    "\n",
    "class PredictionService(object):\n",
    "    tf_model = None\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        if cls.tf_model is None:\n",
    "            cls.tf_model = load_model()\n",
    "        return cls.tf_model\n",
    "\n",
    "    @classmethod\n",
    "    def predict(cls, input):\n",
    "        tf_model = cls.get_model()\n",
    "        return tf_model.predict(input)\n",
    "\n",
    "def load_model():\n",
    "    model = tf.keras.models.load_model(os.path.join(model_path, \"model.h5\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def sigterm_handler(nginx_pid, gunicorn_pid):\n",
    "    try:\n",
    "        os.kill(nginx_pid, signal.SIGQUIT)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.kill(gunicorn_pid, signal.SIGTERM)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "def start_server(timeout, workers):\n",
    "    print(f\"Starting the inference server with {model_server_workers} workers\")\n",
    "    subprocess.check_call([\"ln\", \"-sf\", \"/dev/stdout\", \"/var/log/nginx/access.log\"])\n",
    "    subprocess.check_call([\"ln\", \"-sf\", \"/dev/stderr\", \"/var/log/nginx/error.log\"])\n",
    "    nginx = subprocess.Popen([\"nginx\", \"-c\", \"/opt/program/nginx.conf\"])\n",
    "    gunicorn = subprocess.Popen([\"gunicorn\",\n",
    "                                 \"--timeout\", str(timeout),\n",
    "                                 \"-k\", \"gevent\",\n",
    "                                 \"-b\", \"unix:/tmp/gunicorn.sock\",\n",
    "                                 \"-w\", str(workers),\n",
    "                                 \"wsgi:app\"])\n",
    "\n",
    "    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n",
    "    pids = set([nginx.pid, gunicorn.pid])\n",
    "    while True:\n",
    "        pid, _ = os.wait()\n",
    "        if pid in pids:\n",
    "            break\n",
    "    sigterm_handler(nginx.pid, gunicorn.pid)\n",
    "    print(\"Inference server exiting\")\n",
    "\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/ping\", methods=[\"GET\"])\n",
    "def ping():\n",
    "    health = PredictionService.get_model() is not None\n",
    "    status = 200 if health else 404\n",
    "    return flask.Response(response=\"\\n\", status=status, mimetype=\"application/json\")\n",
    "\n",
    "\n",
    "@app.route(\"/invocations\", methods=[\"POST\"])\n",
    "def invoke():\n",
    "    data = None\n",
    "    if flask.request.content_type == \"text/csv\":\n",
    "        payload = np.fromstring(flask.request.data.decode('utf-8'), sep=\",\")\n",
    "        data = payload.reshape(1, -1)\n",
    "    else:\n",
    "        return flask.Response(response=\"Invalid request data type, only 'text/csv' is supported.\", status=415, mimetype=\"text/plain\")\n",
    "    predictions = PredictionService.predict(data)\n",
    "    out = io.StringIO()\n",
    "    pd.DataFrame({\"results\": predictions.flatten()}).to_csv(out, header=False, index=False)\n",
    "    result = out.getvalue()\n",
    "    print(f\"Prediction Result: {result}\")\n",
    "    return flask.Response(response=result, status=200, mimetype=\"text/csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\", \"preprocess\", \"evaluate\"] ):\n",
    "        raise Exception(\"Invalid argument: you must specify 'train' for training mode, 'serve' for predicting mode, 'preprocess' for preprocessing mode or 'evaluate' for evaluation mode.\") \n",
    "    preprocess = sys.argv[1] == \"preprocess\"\n",
    "    train = sys.argv[1] == \"train\"\n",
    "    evaluate = sys.argv[1] == \"evaluate\"\n",
    "    if preprocess:\n",
    "        model.preprocess()\n",
    "    elif train:\n",
    "        model.train()\n",
    "    elif evaluate:\n",
    "        model.evaluate()\n",
    "    else:\n",
    "        cpu_count = multiprocessing.cpu_count()\n",
    "        model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\n",
    "        model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n",
    "        start_server(model_server_timeout, model_server_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nginx Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nginx.conf\n",
    "worker_processes 1;\n",
    "daemon off;\n",
    "\n",
    "pid /tmp/nginx.pid;\n",
    "error_log /var/log/nginx/error.log;\n",
    "\n",
    "events {\n",
    "\n",
    "}\n",
    "\n",
    "http {\n",
    "  include /etc/nginx/mime.types;\n",
    "  default_type application/octet-stream;\n",
    "  access_log /var/log/nginx/access.log combined;\n",
    "  \n",
    "  upstream gunicorn {\n",
    "    server unix:/tmp/gunicorn.sock;\n",
    "  }\n",
    "\n",
    "  server {\n",
    "\n",
    "    listen 8080 deferred;\n",
    "    client_max_body_size 5m;\n",
    "\n",
    "    keepalive_timeout 5;\n",
    "\n",
    "    location ~ ^/(ping|invocations) {\n",
    "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "      proxy_set_header Host $http_host;\n",
    "      proxy_redirect off;\n",
    "      proxy_pass http://gunicorn;\n",
    "    }\n",
    "\n",
    "    location / {\n",
    "      return 404 \"{}\";\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Server Gateay Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsgi.py\n",
    "import app as myapp\n",
    "app = myapp.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/tensorflow-training:2.5.0-cpu-py37-ubuntu18.04\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    nginx &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir --upgrade \\\n",
    "    flask \\\n",
    "    gevent \\\n",
    "    gunicorn\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "COPY app.py /opt/program\n",
    "COPY model.py /opt/program\n",
    "COPY nginx.conf /opt/program\n",
    "COPY wsgi.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"app.py\"]"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
